{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Wing Wing","title":"Home"},{"location":"#wing","text":"Wing","title":"Wing"},{"location":"p1/overview/","text":"LSM Basic The code is in src/storage/lsm . The architecture of the LSM-tree in Wing is as follows. DBImpl : At the highest level, the DBImpl is the primary component responsible for interacting with users. It references the most recent SuperVersion of the database. It is in lsm.hpp . SuperVersion : It includes a MemTable, a list of immutable MemTables and the on-disk LSM-tree Version , which together represent the current state of the database. It is in version.hpp . MemTable : An in-memory ordered data structure. It is flushed to disk when it reaches its capacity. It is in memtable.hpp . Version : An array of levels, representing the on-disk LSM-tree. It is in version.hpp . Level : It is composed of one or more sorted runs. It is in level.hpp . SortedRun : It can be viewed as a sorted key-value array which is divided into several SSTables. It is in level.hpp . SSTable : It is composed of data blocks, an index, bloom filter and metadata. It is in sst.hpp . Block : It stores records. It is in block.hpp . Record Format Records in the database are structured as (key, seq, type, value) tuple, where seq denotes the timestamp (or sequence number) and type denotes the record type. A record with type=RecordType::Value represents the key-value pair at the timestamp seq . When type equals to RecordType::Deletion , the record indicates that the key has been marked for deletion at the timestamp seq . (key, seq, type) is called the internal key of record (see storage/lsm/format.hpp ) because seq and type are invisible to users. We can define a comparing function to sort them: for two internal keys (key0, seq0, type0) and (key1, seq1, type1) , the former is smaller than the latter if and only if key0 < key1 or key0 == key1 && seq0 > seq1 . With this comparing function, for (key, seq) , the newest record with the same key and a sequence number seq0 <= seq is the first record (key0, seq0) with (key, seq) <= (key0, seq0) . Get DBImpl::Get(key, seq, &value_str) function is designed to retrieve records from the database. It looks for the first record (key0, seq0, type0) satisfying key0 == key && seq0 <= seq . If the record is a record with type RecordType::Deletion or no such record exists, the function returns false, indicating the requested value is not found. If a record with type RecordType::Value is found, the function returns true and copies the associated value into value_str . The process of Get is: Check if MemTable has the record. If not, check if immutable MemTables have the record. It checks from the newest immutable MemTable to the oldest immutable MemTable. It stops once it finds the record. If they do not have the record, proceed to check the on-disk LSM-tree. Check if Level 0 has the record, then Level 1, 2, and so on. For each level, it performs a binary search to find the SSTable that possibly has the record. Then it queries the bloom filter. If further inquiry is necessary, it performs a binary search on the SSTable's index to find the data block. It reads the data block from the disk, and performs a binary search to find the record. Put and Delete DBImpl::Put(key, seq) creates a new record (key, seq, RecordType::Value, value) and DBImpl::Del(key, seq) creates a new record (key, seq, RecordType::Deletion) . Once a record is created, it is inserted to the MemTable. For convenience, a lock is employed to ensure that only a single writer can perform operations at any given time. If the MemTable reaches its capacity, it creates a new superversion and moves the MemTable to the immutable MemTable list and create a new MemTable. The database has 2 threads to persist the data to disk: the flush thread and the compaction thread. The flush thread is awakened whenever a new immutable MemTable is created. It flushes the immutable MemTables to the first level (Level 0) of the LSM-tree. Every time an immutable Memtable is flushed, the compaction thread is awakened. It acquires new compaction tasks through CompactionPicker::Get . Once it has tasks, it proceeds to execute them and subsequently updates the superversion. Scan The database supports range scans. DBImpl::Begin() returns an iterator positioned to the beginning of the data, while DBImpl::Seek(key, seq) returns an iterator positioned to the first record (key0, seq0, type0) satisfying (key, seq) <= (key0, seq0) . Scan operations are performed in a snapshot. When a DBIterator is created, it stores the current sequence number. It can only see the records with sequence number smaller than the stored sequence number. The architecture of iterators is as follows. BlockIterator is the iterator on data blocks. SSTableIterator is the iterator on SSTables and contains a BlockIterator . SortedRunIterator is the iterator on sorted runs and contains a SSTableIterator . SuperVersionIterator is the iterator on superversions, it contains all the SortedRunIterator s and MemTableIterator s using IteratorHeap , which maintains the record with the minimum internal key by maintaining iterators in a heap. There is no LevelIterator or VersionIterator because it is inefficient to maintain two IteratorHeap s. The DBIterator operates at the highest level, merging records with the same key and skipping the keys which are marked deleted. The interfaces of Iterator can be found in storage/lsm/iterator.hpp . Here is an example of usage: DBIterator it = ...; // create the iterator while (it.Valid()) { // Read key and value of the current entry auto key = it.key(); auto value = it.value(); // output key and value DB_INFO(\"{}, {}\", key, value); // Move to the next entry it.Next(); // After moving, key and value may be invalid because they are `Slice`s, // i.e. references to a internal buffer in `it`. }","title":"Overview"},{"location":"p1/overview/#lsm-basic","text":"The code is in src/storage/lsm . The architecture of the LSM-tree in Wing is as follows. DBImpl : At the highest level, the DBImpl is the primary component responsible for interacting with users. It references the most recent SuperVersion of the database. It is in lsm.hpp . SuperVersion : It includes a MemTable, a list of immutable MemTables and the on-disk LSM-tree Version , which together represent the current state of the database. It is in version.hpp . MemTable : An in-memory ordered data structure. It is flushed to disk when it reaches its capacity. It is in memtable.hpp . Version : An array of levels, representing the on-disk LSM-tree. It is in version.hpp . Level : It is composed of one or more sorted runs. It is in level.hpp . SortedRun : It can be viewed as a sorted key-value array which is divided into several SSTables. It is in level.hpp . SSTable : It is composed of data blocks, an index, bloom filter and metadata. It is in sst.hpp . Block : It stores records. It is in block.hpp .","title":"LSM Basic"},{"location":"p1/overview/#record-format","text":"Records in the database are structured as (key, seq, type, value) tuple, where seq denotes the timestamp (or sequence number) and type denotes the record type. A record with type=RecordType::Value represents the key-value pair at the timestamp seq . When type equals to RecordType::Deletion , the record indicates that the key has been marked for deletion at the timestamp seq . (key, seq, type) is called the internal key of record (see storage/lsm/format.hpp ) because seq and type are invisible to users. We can define a comparing function to sort them: for two internal keys (key0, seq0, type0) and (key1, seq1, type1) , the former is smaller than the latter if and only if key0 < key1 or key0 == key1 && seq0 > seq1 . With this comparing function, for (key, seq) , the newest record with the same key and a sequence number seq0 <= seq is the first record (key0, seq0) with (key, seq) <= (key0, seq0) .","title":"Record Format"},{"location":"p1/overview/#get","text":"DBImpl::Get(key, seq, &value_str) function is designed to retrieve records from the database. It looks for the first record (key0, seq0, type0) satisfying key0 == key && seq0 <= seq . If the record is a record with type RecordType::Deletion or no such record exists, the function returns false, indicating the requested value is not found. If a record with type RecordType::Value is found, the function returns true and copies the associated value into value_str . The process of Get is: Check if MemTable has the record. If not, check if immutable MemTables have the record. It checks from the newest immutable MemTable to the oldest immutable MemTable. It stops once it finds the record. If they do not have the record, proceed to check the on-disk LSM-tree. Check if Level 0 has the record, then Level 1, 2, and so on. For each level, it performs a binary search to find the SSTable that possibly has the record. Then it queries the bloom filter. If further inquiry is necessary, it performs a binary search on the SSTable's index to find the data block. It reads the data block from the disk, and performs a binary search to find the record.","title":"Get"},{"location":"p1/overview/#put-and-delete","text":"DBImpl::Put(key, seq) creates a new record (key, seq, RecordType::Value, value) and DBImpl::Del(key, seq) creates a new record (key, seq, RecordType::Deletion) . Once a record is created, it is inserted to the MemTable. For convenience, a lock is employed to ensure that only a single writer can perform operations at any given time. If the MemTable reaches its capacity, it creates a new superversion and moves the MemTable to the immutable MemTable list and create a new MemTable. The database has 2 threads to persist the data to disk: the flush thread and the compaction thread. The flush thread is awakened whenever a new immutable MemTable is created. It flushes the immutable MemTables to the first level (Level 0) of the LSM-tree. Every time an immutable Memtable is flushed, the compaction thread is awakened. It acquires new compaction tasks through CompactionPicker::Get . Once it has tasks, it proceeds to execute them and subsequently updates the superversion.","title":"Put and Delete"},{"location":"p1/overview/#scan","text":"The database supports range scans. DBImpl::Begin() returns an iterator positioned to the beginning of the data, while DBImpl::Seek(key, seq) returns an iterator positioned to the first record (key0, seq0, type0) satisfying (key, seq) <= (key0, seq0) . Scan operations are performed in a snapshot. When a DBIterator is created, it stores the current sequence number. It can only see the records with sequence number smaller than the stored sequence number. The architecture of iterators is as follows. BlockIterator is the iterator on data blocks. SSTableIterator is the iterator on SSTables and contains a BlockIterator . SortedRunIterator is the iterator on sorted runs and contains a SSTableIterator . SuperVersionIterator is the iterator on superversions, it contains all the SortedRunIterator s and MemTableIterator s using IteratorHeap , which maintains the record with the minimum internal key by maintaining iterators in a heap. There is no LevelIterator or VersionIterator because it is inefficient to maintain two IteratorHeap s. The DBIterator operates at the highest level, merging records with the same key and skipping the keys which are marked deleted. The interfaces of Iterator can be found in storage/lsm/iterator.hpp . Here is an example of usage: DBIterator it = ...; // create the iterator while (it.Valid()) { // Read key and value of the current entry auto key = it.key(); auto value = it.value(); // output key and value DB_INFO(\"{}, {}\", key, value); // Move to the next entry it.Next(); // After moving, key and value may be invalid because they are `Slice`s, // i.e. references to a internal buffer in `it`. }","title":"Scan"},{"location":"p1/part1/","text":"Part 1 In part 1, you will implement BlockIterator , BlockBuilder , SSTable , SSTableIterator and SSTableBuilder . SortedRun , SortedRunIterator . IteratorHeap . SuperVersion , SuperVersionIterator and Version . Each class contains some incomplete methods. You may add fields and methods as needed, but do not remove any existing fields or remove public methods, nor change the names of the methods. Block Block stores internal keys and values. If you do not have time to design complex formats such as a compressed format, we highly recommend you to implement the simplest format as follows: where the block is divided into two parts, the first part consists of key-value pairs, while the second part is an array of offsets to the key-value pairs. The key length and value length should be stored in 4 bytes (refer to offset_t in lsm/common.hpp ) or less. BlockIterator takes a pointer to the beginning of the Block (which is stored in a buffer) and the BlockHandle of the Block . You can obtain useful information such as the number of key-value pairs and the size of the Block in BlockHandle . BlockIterator::Seek(user_key, seq) finds the first record larger than (user_key, seq) and moves to it (refer to the definition of the comparison operator in storage/lsm/format.hpp ). BlockIterator::SeekToFirst moves the iterator to the beginning. You can find the comments for Next() , key() , value() and Valid() in storage/lsm/iterator.hpp . BlockBuilder writes key-value pairs to the block until the block reaches the capacity (refer to block_size in storage/lsm/options.hpp , this parameter is passed in the constructor of BlockBuilder ). BlockBuilder::Append returns false if the block is full. You need to ensure that the size of the block do not exceed block_size . The offsets of key-value pairs are written after all the key-value pairs are written and BlockBuilder::FinishBlock is called, so you need to record the offsets while writing the data. The code is in block.hpp and block.cpp . Slice Note that `Slice is equivalent to std::string_view (C++17), which is a reference to a string. It does not allocate a std::string . You need to ensure that the referenced string is not incorrectly deallocated or modified. Test You can test it through test/test_lsm --gtest_filter=LSMTest.BlockTest SSTable You can implement the format as follows: where data blocks are Block . The index consists of the last key and the BlockHandle of each Block , as defined in IndexValue in storage/lsm/format.hpp . It is utilized to locate data block in SSTable::Get , SSTable::Seek and SSTableIterator::Seek . It is preloaded to the memory when opening the SSTable. The bloom filter is used to test whether a key may exist in the SSTable during SSTable::Get . It is also preloaded to the memory. You will implement SSTableBuilder::Append and SSTableBuilder::Finish while maintaining the information about the SSTable: index_data_ (the index data), index_offset_ (the offset of the index block), bloom_filter_offset_ (the offset of the bloom filter), largest_key_ and smallest_key_ (which represent the key range of the SSTable), key_hashes_ . Once a SSTable is created, the information of the SSTable is transferred to the SSTable structure. You can use BlockBuilder to build data blocks. After writing all the key-value pairs, you can write the index data and the metadata to the file. Since we assume that we preload the index data when we open the SSTable, there is no need to use Block to store index data. The code is in sst.cpp and sst.hpp . FileWriter You should use FileWriter to implement SSTableBuilder , BlockBuilder . It collects data and writes them to disk in batch. FileWriter provides two methods WriteValue<T> and WriteString . You can use WriteValue<T> to copy a value of type T to the file. T is a template parameter, it can be uint64_t , float , or structured data which do not have pointers, such as std::pair<uint64_t, uint64_t> and BlockHandle . For string data, you can use WriteString . It only writes the string data, for example, if the string is abc , then it writes 3 bytes a , b and c . Here is an example of usage: std::string str(\"114514\"); writer.WriteValue<uint64_t>(str.length()); .WriteString(str); Note that you should call FileWriter::Flush when all things have been written. If you think the methods of FileWriter is difficult to use, you can modify them, but DO NOT read/write to a raw file handle in SSTableBuilder ! FileReader You can use FileReader to read metadata and index data while initializing SSTable . The method ReadValue and ReadString is similar to WriteValue and WriteString . Here is an example of usage: // Read the string \"114514\" reader.Seek(offset); auto len = reader.ReadValue<uint64_t>(); auto str = reader.ReadString(len); Bloom filter You will build a bloom filter for each SSTable. If you are not familiar with bloom filter, you can read about it in resources such as link . Basically, bloom filter is a bit array. For each key, it set some bits to 1. The positions of these bits are calculated using hash functions. Then, for each key, if all the corresponding bits are 1, the key may exist, otherwise it does not. We have implemented a bloom filter for you, which can be found in common/bloomfilter.hpp and common/bloomfilter.cpp . BloomFilter::Create create a bloom filter. BloomFilter::Add add a key to the bloom filter. Since we only use the hash of keys, you can pass the hash to BloomFilter::Add . BloomFilter::Find checks if a key may exist in the bloom filter. It can also accept the hash of keys. In SSTableBuilder , you should record the hashes of keys in SSTableBuilder::Append and use them to build a bloom filter in SSTableBuilder::Finish . Test You can test it through test/test_lsm --gtest_filter=LSMTest.SSTableTest SortedRun SortedRun stores an array of SSTables. You will implement it based on SSTable and SSTableIterator . The code is in level.cpp and level.hpp . Test You can test it through test/test_lsm --gtest_filter=LSMTest.SortedRunTest IteratorHeap The IteratorHeap structure is used when performing merge-sort on multiple sorted runs. IteratorHeap takes references to the iterators, so you need to store the iterator in another area and pass the reference to it. The usage of IteratorHeap is as follows: First, call IteratorHeap::Push to pass references to the iterators to it. Then, if it is necessary, call IteratorHeap::Build to do some preprocessing. Then, you can use Next , key , value , Valid as an ordinary iterator. It returns the minimum record each time and call Next on the corresponding iterator. We recommend you to use a heap to maintain the minimum record. You can use std::priority_queue or implement your own. The code is in iterator_heap.hpp . Note that IteratorHeap is a template class, if you are not familiar with it, you can read about templates in resources such as link . Test You can test it through test/test_lsm --gtest_filter=LSMTest.IteratorHeapTest SuperVersion After you implement SortedRun , SortedRunIterator and IteratorHeap , implementing SuperVersion should be straightforward. The code is in version.cpp and version.hpp . Test You can test it through test/test_lsm --gtest_filter=LSMTest.SuperVersionTest Smart Pointers We use smart pointers to manage reference counts for SSTables and sorted runs. If you are not familiar with them, you can read about smart pointers in resources such as link . DO NOT delete them! We rely on reference counts to support multiversion concurrency control. Submit make submit in the build directory to create submission.zip and submit it to autolab.","title":"Part 1"},{"location":"p1/part1/#part-1","text":"In part 1, you will implement BlockIterator , BlockBuilder , SSTable , SSTableIterator and SSTableBuilder . SortedRun , SortedRunIterator . IteratorHeap . SuperVersion , SuperVersionIterator and Version . Each class contains some incomplete methods. You may add fields and methods as needed, but do not remove any existing fields or remove public methods, nor change the names of the methods.","title":"Part 1"},{"location":"p1/part1/#block","text":"Block stores internal keys and values. If you do not have time to design complex formats such as a compressed format, we highly recommend you to implement the simplest format as follows: where the block is divided into two parts, the first part consists of key-value pairs, while the second part is an array of offsets to the key-value pairs. The key length and value length should be stored in 4 bytes (refer to offset_t in lsm/common.hpp ) or less. BlockIterator takes a pointer to the beginning of the Block (which is stored in a buffer) and the BlockHandle of the Block . You can obtain useful information such as the number of key-value pairs and the size of the Block in BlockHandle . BlockIterator::Seek(user_key, seq) finds the first record larger than (user_key, seq) and moves to it (refer to the definition of the comparison operator in storage/lsm/format.hpp ). BlockIterator::SeekToFirst moves the iterator to the beginning. You can find the comments for Next() , key() , value() and Valid() in storage/lsm/iterator.hpp . BlockBuilder writes key-value pairs to the block until the block reaches the capacity (refer to block_size in storage/lsm/options.hpp , this parameter is passed in the constructor of BlockBuilder ). BlockBuilder::Append returns false if the block is full. You need to ensure that the size of the block do not exceed block_size . The offsets of key-value pairs are written after all the key-value pairs are written and BlockBuilder::FinishBlock is called, so you need to record the offsets while writing the data. The code is in block.hpp and block.cpp .","title":"Block"},{"location":"p1/part1/#slice","text":"Note that `Slice is equivalent to std::string_view (C++17), which is a reference to a string. It does not allocate a std::string . You need to ensure that the referenced string is not incorrectly deallocated or modified.","title":"Slice"},{"location":"p1/part1/#test","text":"You can test it through test/test_lsm --gtest_filter=LSMTest.BlockTest","title":"Test"},{"location":"p1/part1/#sstable","text":"You can implement the format as follows: where data blocks are Block . The index consists of the last key and the BlockHandle of each Block , as defined in IndexValue in storage/lsm/format.hpp . It is utilized to locate data block in SSTable::Get , SSTable::Seek and SSTableIterator::Seek . It is preloaded to the memory when opening the SSTable. The bloom filter is used to test whether a key may exist in the SSTable during SSTable::Get . It is also preloaded to the memory. You will implement SSTableBuilder::Append and SSTableBuilder::Finish while maintaining the information about the SSTable: index_data_ (the index data), index_offset_ (the offset of the index block), bloom_filter_offset_ (the offset of the bloom filter), largest_key_ and smallest_key_ (which represent the key range of the SSTable), key_hashes_ . Once a SSTable is created, the information of the SSTable is transferred to the SSTable structure. You can use BlockBuilder to build data blocks. After writing all the key-value pairs, you can write the index data and the metadata to the file. Since we assume that we preload the index data when we open the SSTable, there is no need to use Block to store index data. The code is in sst.cpp and sst.hpp .","title":"SSTable"},{"location":"p1/part1/#filewriter","text":"You should use FileWriter to implement SSTableBuilder , BlockBuilder . It collects data and writes them to disk in batch. FileWriter provides two methods WriteValue<T> and WriteString . You can use WriteValue<T> to copy a value of type T to the file. T is a template parameter, it can be uint64_t , float , or structured data which do not have pointers, such as std::pair<uint64_t, uint64_t> and BlockHandle . For string data, you can use WriteString . It only writes the string data, for example, if the string is abc , then it writes 3 bytes a , b and c . Here is an example of usage: std::string str(\"114514\"); writer.WriteValue<uint64_t>(str.length()); .WriteString(str); Note that you should call FileWriter::Flush when all things have been written. If you think the methods of FileWriter is difficult to use, you can modify them, but DO NOT read/write to a raw file handle in SSTableBuilder !","title":"FileWriter"},{"location":"p1/part1/#filereader","text":"You can use FileReader to read metadata and index data while initializing SSTable . The method ReadValue and ReadString is similar to WriteValue and WriteString . Here is an example of usage: // Read the string \"114514\" reader.Seek(offset); auto len = reader.ReadValue<uint64_t>(); auto str = reader.ReadString(len);","title":"FileReader"},{"location":"p1/part1/#bloom-filter","text":"You will build a bloom filter for each SSTable. If you are not familiar with bloom filter, you can read about it in resources such as link . Basically, bloom filter is a bit array. For each key, it set some bits to 1. The positions of these bits are calculated using hash functions. Then, for each key, if all the corresponding bits are 1, the key may exist, otherwise it does not. We have implemented a bloom filter for you, which can be found in common/bloomfilter.hpp and common/bloomfilter.cpp . BloomFilter::Create create a bloom filter. BloomFilter::Add add a key to the bloom filter. Since we only use the hash of keys, you can pass the hash to BloomFilter::Add . BloomFilter::Find checks if a key may exist in the bloom filter. It can also accept the hash of keys. In SSTableBuilder , you should record the hashes of keys in SSTableBuilder::Append and use them to build a bloom filter in SSTableBuilder::Finish .","title":"Bloom filter"},{"location":"p1/part1/#test_1","text":"You can test it through test/test_lsm --gtest_filter=LSMTest.SSTableTest","title":"Test"},{"location":"p1/part1/#sortedrun","text":"SortedRun stores an array of SSTables. You will implement it based on SSTable and SSTableIterator . The code is in level.cpp and level.hpp .","title":"SortedRun"},{"location":"p1/part1/#test_2","text":"You can test it through test/test_lsm --gtest_filter=LSMTest.SortedRunTest","title":"Test"},{"location":"p1/part1/#iteratorheap","text":"The IteratorHeap structure is used when performing merge-sort on multiple sorted runs. IteratorHeap takes references to the iterators, so you need to store the iterator in another area and pass the reference to it. The usage of IteratorHeap is as follows: First, call IteratorHeap::Push to pass references to the iterators to it. Then, if it is necessary, call IteratorHeap::Build to do some preprocessing. Then, you can use Next , key , value , Valid as an ordinary iterator. It returns the minimum record each time and call Next on the corresponding iterator. We recommend you to use a heap to maintain the minimum record. You can use std::priority_queue or implement your own. The code is in iterator_heap.hpp . Note that IteratorHeap is a template class, if you are not familiar with it, you can read about templates in resources such as link .","title":"IteratorHeap"},{"location":"p1/part1/#test_3","text":"You can test it through test/test_lsm --gtest_filter=LSMTest.IteratorHeapTest","title":"Test"},{"location":"p1/part1/#superversion","text":"After you implement SortedRun , SortedRunIterator and IteratorHeap , implementing SuperVersion should be straightforward. The code is in version.cpp and version.hpp .","title":"SuperVersion"},{"location":"p1/part1/#test_4","text":"You can test it through test/test_lsm --gtest_filter=LSMTest.SuperVersionTest","title":"Test"},{"location":"p1/part1/#smart-pointers","text":"We use smart pointers to manage reference counts for SSTables and sorted runs. If you are not familiar with them, you can read about smart pointers in resources such as link . DO NOT delete them! We rely on reference counts to support multiversion concurrency control.","title":"Smart Pointers"},{"location":"p1/part1/#submit","text":"make submit in the build directory to create submission.zip and submit it to autolab.","title":"Submit"},{"location":"p1/part2/","text":"Part 2 In part 2, you will implement compaction mechanism. You will implement: CompactionJob . This component receives an iterator and persists its output as a list of SSTables. It is utilized in both DBImpl::FlushThread and DBImpl::CompactionThread . LeveledCompactionPicker , which is derived from CompactionPicker . It generates a new compaction task. The parameters for the compaction task are stored in Compaction . It returns nullptr if no compaction is required for the LSM-tree. DBImpl::CompactionThread . This thread manages the compaction process and updates the superversion. You may refer to the implementation of DBImpl::FlushThread for guidance. You may need to use ulimit to increase the number of open file descriptors. (by default it is 1024, which is not enough). DBImpl::CompactionJob You will implement CompactionJob::Run. It receives an iterator and writes the iterator's output to disk. You should merge the records with the same key. The output is divided into SSTables, with the size of data blocks in SSTable not exceeding the target SSTable size (refer to sst_file_size in storage/lsm/options.hpp ). The actual SSTable size may be larger than the target SSTable size since we have index data, bloom filter and metadata. The code is in compaction_job.hpp . Test You can test it through test/test_lsm --gtest_filter=LSMTest.CompactionBasicTest Leveled compaction strategy You will implement the leveled compaction strategy. This strategy is the default strategy of RocksDB. The leveled compaction strategy in Wing is as follows: Suppose the size ratio is T T , and the size limit of Level 0 is B B . The size limit of Level 1, 2, 3 \\cdots \\cdots is BT,BT^2,BT^3\\cdots BT,BT^2,BT^3\\cdots Level 0 consists of multiple sorted runs flushed by DBImpl::FlushThread , each sorted run has 1 or 2 SSTables. If the number of sorted runs is larger than 4, all the sorted runs are compacted to the next level. If the number of sorted runs reaches 20, then DBImpl::FlushThread stops to flush until they are compacted to the next level. Level 1, 2, 3 \\cdots \\cdots consists of only one sorted run. If one of them reaches its capacity, it picks an SSTable with the minimum overlapping with the next level, and compact the SSTable with the next level. In LeveledCompactionPicker::Get, you must check for the existence of a compaction task. If one exists, you must create and return a std::unique_ptr<Compaction> . Otherwise, return a nullptr. The code is in compaction_picker.hpp , compaction_picker.cpp , compaction.hpp . DBImpl::CompactionThread Put/Del operation The Put/Del operation inserts a record into the LSM-tree. The key is converted to an internal key by adding the current sequence number and record type ( RecordType::Value or RecordType::Deletion ). Initially, it is inserted into the memtable. If the memtable reaches its capacity, it is appended to the list of immutable memtables. Then it nofities the flush thread. Then it installs the superversion (refer to DBImpl::SwitchMemtable ). The flush thread waits for the signal and fetches the list of immutable memtables. It flushes them (through CompactionJob ) to sorted runs. Once the sorted runs are created, it creates a new superversion, installs the superversion, and notifies the compaction thread (refer to DBImpl::FlushThread ). The compaction thread waits for the signal from the flush thread. Upon awakening, it checks the levels and attempts to find a compaction task (refer to CompactionPicker::Get and Compaction in lsm/compaction_pick.hpp and lsm/compaction.hpp ). For example, it may find that Level 0 is too large, so it creates a compaction task: compacting some SSTable files from Level 0 to Level 1. After new SSTable files are created, it creates a new superversion, installs the superversion and looks for compaction tasks again. It is possible that multiple compactions occur when one immutable memtable is flushed. It also add removal tags to useless SSTables, so that they will be removed while destruction. Locks in LSM-tree There are 3 locks in LSM-tree. We use concurrency primitives in C++, including std::mutex , std::unique_lock , std::shared_mutex (C++17), std::shared_lock (C++17), std::condition_variable . The first is db_mutex_ . This mutex is used to protect the process of operating metadata (e.g. switching memtables, creating new superversion and installing new superversions). The second is write_mutex_ . This mutex is used to protect Put operations. The third is sv_mutex_ . This mutex is used to protect the reference and installation of the superversion pointer. Multiversion Concurrency Control in LSM-tree In LSM-tree, Put operations do not block Get / Scan (implemented by DBIterator ) operations. This is achieved by supporting multiple superversions simultaneously. Each superversion has a reference count which is maintained by std::shared_ptr , the SSTables and the sorted runs in one superversion will not be removed if the reference count does not decrease to 0. For Get and Scan operations, a superversion is referenced, and the data accessed within this superversion remains consistent regardless of new superversions created by the Put / FlushThread / CompactionThread . Task You will implement DBImpl::CompactionThread . The compaction thread awaits signals from the flush thread or the deconstructor via the condition variable compact_cv_ . Upon being awakened, it first checks stop_signal_ . If stop_signal_ is true, it stops immediately. Otherwise, it calls CompactionPicker::Get to obtain a compaction task. If a task is present, it executes the compaction outside of the DB mutex. After compaction, it reacquires the DB mutex, creates a new superversion, and updates the DBImpl superversion. You can assume that the number of SSTables is small, allowing you to iterate through each SSTable and copy their pointers to the new superversion. You also need to set compaction_flag_ to true if you are not waiting for compact_cv_ , like flush_flag_ in DBImpl::FlushThread . More specifically, you need to write something like: void DBImpl::CompactionThread() { while (!stop_signal_) { std::unique_lock lck(db_mutex_); // Check if it has to stop. // It has to stop when the LSM-tree shutdowns. if (stop_signal_) { compact_flag_ = false; return; } std::unique_ptr<Compaction> compaction = /* A new compaction task */ if (!compaction) { compact_flag_ = false; compact_cv_.wait(lck); continue; } compact_flag_ = true; // Do some other things db_mutex_.unlock(); // Do compaction db_mutex_.lock(); // Create a new superversion and install it } } The DB mutex should only be used for metadata operations. You must avoid performing the compaction process under the DB mutex. The code is in lsm.cpp and lsm.hpp . Remove old SSTables Utilize SetRemoveTag to set the remove_tag_ to true. When the destructor of SSTable is invoked, it checks the tag and determines whether to remove the file. This is safe as the destructor is called only when the SSTable is not in use. Test You can test all the components through test/test_lsm --gtest_filter=LSMTest.LSMBasicTest:LSMTest.LSMSmallGetTest:LSMTest.LSMSmallScanTest:LSMTest.LSMSmallMultithreadGetPutTest:LSMTest.LSMSaveTest:LSMTest.LSMBigScanTest:LSMTest.LSMDuplicateKeyTest:LSMTest.LeveledCompactionTest or, you can just use test/test_lsm --gtest_filter=LSMTest.LSM*:LSMTest.LeveledCompactionTest","title":"Part 2"},{"location":"p1/part2/#part-2","text":"In part 2, you will implement compaction mechanism. You will implement: CompactionJob . This component receives an iterator and persists its output as a list of SSTables. It is utilized in both DBImpl::FlushThread and DBImpl::CompactionThread . LeveledCompactionPicker , which is derived from CompactionPicker . It generates a new compaction task. The parameters for the compaction task are stored in Compaction . It returns nullptr if no compaction is required for the LSM-tree. DBImpl::CompactionThread . This thread manages the compaction process and updates the superversion. You may refer to the implementation of DBImpl::FlushThread for guidance. You may need to use ulimit to increase the number of open file descriptors. (by default it is 1024, which is not enough).","title":"Part 2"},{"location":"p1/part2/#dbimplcompactionjob","text":"You will implement CompactionJob::Run. It receives an iterator and writes the iterator's output to disk. You should merge the records with the same key. The output is divided into SSTables, with the size of data blocks in SSTable not exceeding the target SSTable size (refer to sst_file_size in storage/lsm/options.hpp ). The actual SSTable size may be larger than the target SSTable size since we have index data, bloom filter and metadata. The code is in compaction_job.hpp .","title":"DBImpl::CompactionJob"},{"location":"p1/part2/#test","text":"You can test it through test/test_lsm --gtest_filter=LSMTest.CompactionBasicTest","title":"Test"},{"location":"p1/part2/#leveled-compaction-strategy","text":"You will implement the leveled compaction strategy. This strategy is the default strategy of RocksDB. The leveled compaction strategy in Wing is as follows: Suppose the size ratio is T T , and the size limit of Level 0 is B B . The size limit of Level 1, 2, 3 \\cdots \\cdots is BT,BT^2,BT^3\\cdots BT,BT^2,BT^3\\cdots Level 0 consists of multiple sorted runs flushed by DBImpl::FlushThread , each sorted run has 1 or 2 SSTables. If the number of sorted runs is larger than 4, all the sorted runs are compacted to the next level. If the number of sorted runs reaches 20, then DBImpl::FlushThread stops to flush until they are compacted to the next level. Level 1, 2, 3 \\cdots \\cdots consists of only one sorted run. If one of them reaches its capacity, it picks an SSTable with the minimum overlapping with the next level, and compact the SSTable with the next level. In LeveledCompactionPicker::Get, you must check for the existence of a compaction task. If one exists, you must create and return a std::unique_ptr<Compaction> . Otherwise, return a nullptr. The code is in compaction_picker.hpp , compaction_picker.cpp , compaction.hpp .","title":"Leveled compaction strategy"},{"location":"p1/part2/#dbimplcompactionthread","text":"","title":"DBImpl::CompactionThread"},{"location":"p1/part2/#putdel-operation","text":"The Put/Del operation inserts a record into the LSM-tree. The key is converted to an internal key by adding the current sequence number and record type ( RecordType::Value or RecordType::Deletion ). Initially, it is inserted into the memtable. If the memtable reaches its capacity, it is appended to the list of immutable memtables. Then it nofities the flush thread. Then it installs the superversion (refer to DBImpl::SwitchMemtable ). The flush thread waits for the signal and fetches the list of immutable memtables. It flushes them (through CompactionJob ) to sorted runs. Once the sorted runs are created, it creates a new superversion, installs the superversion, and notifies the compaction thread (refer to DBImpl::FlushThread ). The compaction thread waits for the signal from the flush thread. Upon awakening, it checks the levels and attempts to find a compaction task (refer to CompactionPicker::Get and Compaction in lsm/compaction_pick.hpp and lsm/compaction.hpp ). For example, it may find that Level 0 is too large, so it creates a compaction task: compacting some SSTable files from Level 0 to Level 1. After new SSTable files are created, it creates a new superversion, installs the superversion and looks for compaction tasks again. It is possible that multiple compactions occur when one immutable memtable is flushed. It also add removal tags to useless SSTables, so that they will be removed while destruction.","title":"Put/Del operation"},{"location":"p1/part2/#locks-in-lsm-tree","text":"There are 3 locks in LSM-tree. We use concurrency primitives in C++, including std::mutex , std::unique_lock , std::shared_mutex (C++17), std::shared_lock (C++17), std::condition_variable . The first is db_mutex_ . This mutex is used to protect the process of operating metadata (e.g. switching memtables, creating new superversion and installing new superversions). The second is write_mutex_ . This mutex is used to protect Put operations. The third is sv_mutex_ . This mutex is used to protect the reference and installation of the superversion pointer.","title":"Locks in LSM-tree"},{"location":"p1/part2/#multiversion-concurrency-control-in-lsm-tree","text":"In LSM-tree, Put operations do not block Get / Scan (implemented by DBIterator ) operations. This is achieved by supporting multiple superversions simultaneously. Each superversion has a reference count which is maintained by std::shared_ptr , the SSTables and the sorted runs in one superversion will not be removed if the reference count does not decrease to 0. For Get and Scan operations, a superversion is referenced, and the data accessed within this superversion remains consistent regardless of new superversions created by the Put / FlushThread / CompactionThread .","title":"Multiversion Concurrency Control in LSM-tree"},{"location":"p1/part2/#task","text":"You will implement DBImpl::CompactionThread . The compaction thread awaits signals from the flush thread or the deconstructor via the condition variable compact_cv_ . Upon being awakened, it first checks stop_signal_ . If stop_signal_ is true, it stops immediately. Otherwise, it calls CompactionPicker::Get to obtain a compaction task. If a task is present, it executes the compaction outside of the DB mutex. After compaction, it reacquires the DB mutex, creates a new superversion, and updates the DBImpl superversion. You can assume that the number of SSTables is small, allowing you to iterate through each SSTable and copy their pointers to the new superversion. You also need to set compaction_flag_ to true if you are not waiting for compact_cv_ , like flush_flag_ in DBImpl::FlushThread . More specifically, you need to write something like: void DBImpl::CompactionThread() { while (!stop_signal_) { std::unique_lock lck(db_mutex_); // Check if it has to stop. // It has to stop when the LSM-tree shutdowns. if (stop_signal_) { compact_flag_ = false; return; } std::unique_ptr<Compaction> compaction = /* A new compaction task */ if (!compaction) { compact_flag_ = false; compact_cv_.wait(lck); continue; } compact_flag_ = true; // Do some other things db_mutex_.unlock(); // Do compaction db_mutex_.lock(); // Create a new superversion and install it } } The DB mutex should only be used for metadata operations. You must avoid performing the compaction process under the DB mutex. The code is in lsm.cpp and lsm.hpp .","title":"Task"},{"location":"p1/part2/#remove-old-sstables","text":"Utilize SetRemoveTag to set the remove_tag_ to true. When the destructor of SSTable is invoked, it checks the tag and determines whether to remove the file. This is safe as the destructor is called only when the SSTable is not in use.","title":"Remove old SSTables"},{"location":"p1/part2/#test_1","text":"You can test all the components through test/test_lsm --gtest_filter=LSMTest.LSMBasicTest:LSMTest.LSMSmallGetTest:LSMTest.LSMSmallScanTest:LSMTest.LSMSmallMultithreadGetPutTest:LSMTest.LSMSaveTest:LSMTest.LSMBigScanTest:LSMTest.LSMDuplicateKeyTest:LSMTest.LeveledCompactionTest or, you can just use test/test_lsm --gtest_filter=LSMTest.LSM*:LSMTest.LeveledCompactionTest","title":"Test"},{"location":"p1/part3/","text":"Part 3: Tradeoff between range scan and compaction You should submit your code and report on the Web Learning \u7f51\u7edc\u5b66\u5802. In the report, you need to write your answers to the problems. Problem 1: lazy leveling (3pts) We previously implemented the leveling compaction policy: there is only one sorted run in each level, and the size of the latter level is T T times the size of the former level. Therefore, the write amplification to compaction one key to the next level is T T . If there are L L levels, the write amplification is TL TL . Tiering is another compaction policy: there are at most T T sorted runs in each level, and when the number of sorted runs in a level reaches T T , all T T sorted runs will be merged (i.e., compacted) into a new sorted run in the next level. Since for each key, it can only be compacted from one level to the next level, a key will be compacted at most L L times if there are L L levels. Thus the write amplification of the tiering policy is O(L) O(L) . It is smaller than the write amplification of leveling strategy O(TL) O(TL) . However, the tiering policy suffers from high space amplification. In the worst case, there can be T T duplicate sorted runs in the last level, and the space amplification can be T T . It is unacceptable. Thus, lazy leveling is proposed. Lazy leveling combines the advantages of the leveling policy and the tiering policy. Suppose there are L L levels in total. Lazy leveling uses the tiering compaction policy in Level 1 1 , Level 2 2 , \\cdots \\cdots , Level L-1 L-1 to reduce the write amplification. The maximum number of sorted runs in these levels is T T . Lazy leveling limits the space amplification by allowing only one sorted run in the last level. When the number of sorted runs in Level 1 1 , 2 2 , \\cdots \\cdots , L-2 L-2 reaches T T , all these sorted runs in the respective level will be merged into a new sorted run in the next level. When the number of sorted runs in Level L-1 L-1 reaches T T , all sorted runs in it will be merged into the sorted run in Level L L . Here we analyze the write amplification of lazy leveling. Similar to the tiering policy, each non-last level contributes 1 1 to the write amplification. The last level uses the leveling compaction policy, therefore it contributes C C to the write amplification, where C C is the size ratio between Level L L and Level L-1 L-1 . Therefore, the total write amplification is L - 1 + C L - 1 + C . The write cost w w is (L - 1 + C)(input\\ size) (L - 1 + C)(input\\ size) . Your tasks are as follows: Implement lazy leveling in LazyLevelingCompactionPicker::Get . You can test it through test/test_lsm --gtest_filter=LSMTest.LazyLevelingCompactionTest . Please submit to autolab. Measure the write amplification in the test and compare it with the theoretical write amplification L - 1 + C L - 1 + C in the report. If they are different, please analyze the reason. Problem 2: find the best compaction policy (4pts) Range scan is a query type that retrieves all records in the given range. To scan a range, we first seek the begin key in all sorted runs, and then sequentially read subsequent records until the end of the range. In problem 2 and problem 3, we only consider short range scan (scan length \\leq 100 \\leq 100 ). We assume that for each sorted run, we read only one block. So the range scan cost for lazy leveling is (1 + T (L-1))(block\\ size) (1 + T (L-1))(block\\ size) , where 1 + T(L-1) 1 + T(L-1) is the number of sorted runs. We can generalize the lazy leveling compaction policy by allowing different maximum numbers of sorted runs in non-last levels. Specifically, for Level i i where i < L i < L , we designate the maximum number of sorted runs as k_i k_i . We model the total cost of compactions and range scans in the generalized lazy leveling policy as f(\\vec k, C) = w(\\vec k, C) + \\alpha r(\\vec k) f(\\vec k, C) = w(\\vec k, C) + \\alpha r(\\vec k) . The write cost w(\\vec k, C) = (L - 1 + C)(input\\ size) w(\\vec k, C) = (L - 1 + C)(input\\ size) . The read cost r(\\vec k) r(\\vec k) is the I/O cost of N N range scans where N N is the number of input keys. You need to model the read cost and write cost. \\alpha \\alpha describes the workload: a small \\alpha \\alpha for a write-heavy workload and a large \\alpha \\alpha for a scan-heavy workload. Your task: given the size of the last level N N , the base level size F F , the workload parameter alpha alpha , find \\vec k \\vec k and C C that minimize f(\\vec k, C) f(\\vec k, C) and satisfy N = \\prod_{i=1}^{L-1} k_i F C N = \\prod_{i=1}^{L-1} k_i F C . You need to design an algorithm to calculate optimal \\vec k, C \\vec k, C based on parameters. More specifically, you need to implement FluidCompactionPicker::Get and adjust the maximum number of sorted runs in each level based on your algorithms. You may explore when and how to adjust the maximum number of sorted runs in each level. For example, you may calculate the optimal \\vec k \\vec k and C C for every 5 seconds and apply the changes only when the optimal \\vec k \\vec k or C C differs much from the current value. We provide a basic benchmark that can be executed by test/test_lsm --gtest_filter=LSMTest.Part3Benchmark1 . Your algorithm should outperform than the baseline. There is no need to submit to autolab due to the long execution time. Please write a report detailing the algorithm you have designed and implemented. Additionally, include a comprehensive comparison with other compaction policies. Compare your algorithm with other compaction policies, including leveling, tiering, and lazy leveling in problem 1. Furthermore, adjust the alpha value in the benchmark (passing different alpha value to Part3Benchmark function), and determine the range of alpha where your algorithm performs the best. The parameters in Part3Benchmark(alpha, N, scan_length) are: alpha value, N is the number of keys, scan_length is the length of range scan. scan_length is set to be larger than N in this problem to ensure that all sorted runs are accessed. The read cost is still calculated by (number of sorted runs)*(block size) regardless of long scan_length . The read cost is calculated in the benchmark function as follows: read cost = 0 write cost = 0 for T in range(10): insert 10% of key-value pairs read cost += (number of sorted runs) * (block size) * (number of inserted key pairs) write cost += (write cost of inserted key pairs) read cost /= 10 write cost /= 10 total cost = (read cost) * alpha + (write cost) You can get points as long as your solution is reasonable and well-founded. Problem 3: find the best compaction policy considering range filters (3pts) Similar to bloom filters, there are also range filters. They can determine whether keys exist within a specified range in a sorted run. It allows range scans to skip sorted runs without relevant keys, optimizing query performance. We assume that we have a perfect range filter which does not produce false positives. Using this range filter, we can calculate the read cost based on the expected number of sorted runs that need to read. We also assume that the length m m of range scan is always the same. Let the total size of LSM-tree be S S . Suppose there is a sorted run of size T T in the LSM-tree. Assuming that all the keys in the LSM-tree are unique and uniformly distributed, for a range scan of length m m , the probability that a key from the sorted run will be in the result of the range scan is 1 - \\prod_{i=0}^{m-1}\\frac{S-T-i}{S-i} 1 - \\prod_{i=0}^{m-1}\\frac{S-T-i}{S-i} . You can consider for the first key in the result of the range scan, the probability that the key is not in the sorted run is \\frac{S-T}{S} \\frac{S-T}{S} , and for the second key it is \\frac{S-T-1}{S-1} \\frac{S-T-1}{S-1} , and so on. This probability is approximately \\approx 1-(1-\\frac{T}{S})^m\\approx 1-e^{-mT/S} \\approx 1-(1-\\frac{T}{S})^m\\approx 1-e^{-mT/S} . Then the read cost can be calculated by r(\\vec k)=(\\sum_{T} 1-e^{-mT/S})(block\\ size) r(\\vec k)=(\\sum_{T} 1-e^{-mT/S})(block\\ size) . The write cost is the same as problem 2. The code of estimation is in Part3Benchmark function. Your task: given the size of the last level N N , the base level size F F , the workload parameter alpha alpha , and the range scan length m m , approximate the read cost and write cost, find \\vec k, C \\vec k, C that minimize f(\\vec k, C) = w(\\vec k, C) + \\alpha r(\\vec k) f(\\vec k, C) = w(\\vec k, C) + \\alpha r(\\vec k) and satisfy N = \\prod_{i=1}^{L-1} k_i C F N = \\prod_{i=1}^{L-1} k_i C F . You need to implement it in FluidCompactionPicker::Get . We provide a basic benchmark that can be executed by test/test_lsm --gtest_filter=LSMTest.Part3Benchmark2 . Your algorithm should outperform than the baseline. There is no need to submit to autolab due to the long execution time. Please write a report detailing the algorithm you have designed and implemented. Additionally, include a comprehensive comparison with other compaction policies. Compare your algorithm with other compaction policies, including leveling, tiering, and lazy leveling in problem 1. Furthermore, adjust the alpha value in the benchmark, and determine the range of alpha where your algorithm performs the best. The parameters in Part3Benchmark(alpha, N, scan_length) are: alpha value, N is the number of keys, scan_length is the length of range scan. scan_length is set to 100 by default. You can also pass different scan_length to this function. The read cost is calculated as follows: read cost = 0 write cost = 0 for T in range(10): insert 10% of key-value pairs read cost += (number of sorted runs that need to be accessed) * (block size) * (number of inserted key pairs) write cost += (write cost of inserted key pairs) read cost /= 10 write cost /= 10 total cost = (read cost) * alpha + (write cost) You can get points as long as your solution is reasonable and well-founded.","title":"Part 3"},{"location":"p1/part3/#part-3-tradeoff-between-range-scan-and-compaction","text":"You should submit your code and report on the Web Learning \u7f51\u7edc\u5b66\u5802. In the report, you need to write your answers to the problems.","title":"Part 3: Tradeoff between range scan and compaction"},{"location":"p1/part3/#problem-1-lazy-leveling-3pts","text":"We previously implemented the leveling compaction policy: there is only one sorted run in each level, and the size of the latter level is T T times the size of the former level. Therefore, the write amplification to compaction one key to the next level is T T . If there are L L levels, the write amplification is TL TL . Tiering is another compaction policy: there are at most T T sorted runs in each level, and when the number of sorted runs in a level reaches T T , all T T sorted runs will be merged (i.e., compacted) into a new sorted run in the next level. Since for each key, it can only be compacted from one level to the next level, a key will be compacted at most L L times if there are L L levels. Thus the write amplification of the tiering policy is O(L) O(L) . It is smaller than the write amplification of leveling strategy O(TL) O(TL) . However, the tiering policy suffers from high space amplification. In the worst case, there can be T T duplicate sorted runs in the last level, and the space amplification can be T T . It is unacceptable. Thus, lazy leveling is proposed. Lazy leveling combines the advantages of the leveling policy and the tiering policy. Suppose there are L L levels in total. Lazy leveling uses the tiering compaction policy in Level 1 1 , Level 2 2 , \\cdots \\cdots , Level L-1 L-1 to reduce the write amplification. The maximum number of sorted runs in these levels is T T . Lazy leveling limits the space amplification by allowing only one sorted run in the last level. When the number of sorted runs in Level 1 1 , 2 2 , \\cdots \\cdots , L-2 L-2 reaches T T , all these sorted runs in the respective level will be merged into a new sorted run in the next level. When the number of sorted runs in Level L-1 L-1 reaches T T , all sorted runs in it will be merged into the sorted run in Level L L . Here we analyze the write amplification of lazy leveling. Similar to the tiering policy, each non-last level contributes 1 1 to the write amplification. The last level uses the leveling compaction policy, therefore it contributes C C to the write amplification, where C C is the size ratio between Level L L and Level L-1 L-1 . Therefore, the total write amplification is L - 1 + C L - 1 + C . The write cost w w is (L - 1 + C)(input\\ size) (L - 1 + C)(input\\ size) . Your tasks are as follows: Implement lazy leveling in LazyLevelingCompactionPicker::Get . You can test it through test/test_lsm --gtest_filter=LSMTest.LazyLevelingCompactionTest . Please submit to autolab. Measure the write amplification in the test and compare it with the theoretical write amplification L - 1 + C L - 1 + C in the report. If they are different, please analyze the reason.","title":"Problem 1: lazy leveling (3pts)"},{"location":"p1/part3/#problem-2-find-the-best-compaction-policy-4pts","text":"Range scan is a query type that retrieves all records in the given range. To scan a range, we first seek the begin key in all sorted runs, and then sequentially read subsequent records until the end of the range. In problem 2 and problem 3, we only consider short range scan (scan length \\leq 100 \\leq 100 ). We assume that for each sorted run, we read only one block. So the range scan cost for lazy leveling is (1 + T (L-1))(block\\ size) (1 + T (L-1))(block\\ size) , where 1 + T(L-1) 1 + T(L-1) is the number of sorted runs. We can generalize the lazy leveling compaction policy by allowing different maximum numbers of sorted runs in non-last levels. Specifically, for Level i i where i < L i < L , we designate the maximum number of sorted runs as k_i k_i . We model the total cost of compactions and range scans in the generalized lazy leveling policy as f(\\vec k, C) = w(\\vec k, C) + \\alpha r(\\vec k) f(\\vec k, C) = w(\\vec k, C) + \\alpha r(\\vec k) . The write cost w(\\vec k, C) = (L - 1 + C)(input\\ size) w(\\vec k, C) = (L - 1 + C)(input\\ size) . The read cost r(\\vec k) r(\\vec k) is the I/O cost of N N range scans where N N is the number of input keys. You need to model the read cost and write cost. \\alpha \\alpha describes the workload: a small \\alpha \\alpha for a write-heavy workload and a large \\alpha \\alpha for a scan-heavy workload. Your task: given the size of the last level N N , the base level size F F , the workload parameter alpha alpha , find \\vec k \\vec k and C C that minimize f(\\vec k, C) f(\\vec k, C) and satisfy N = \\prod_{i=1}^{L-1} k_i F C N = \\prod_{i=1}^{L-1} k_i F C . You need to design an algorithm to calculate optimal \\vec k, C \\vec k, C based on parameters. More specifically, you need to implement FluidCompactionPicker::Get and adjust the maximum number of sorted runs in each level based on your algorithms. You may explore when and how to adjust the maximum number of sorted runs in each level. For example, you may calculate the optimal \\vec k \\vec k and C C for every 5 seconds and apply the changes only when the optimal \\vec k \\vec k or C C differs much from the current value. We provide a basic benchmark that can be executed by test/test_lsm --gtest_filter=LSMTest.Part3Benchmark1 . Your algorithm should outperform than the baseline. There is no need to submit to autolab due to the long execution time. Please write a report detailing the algorithm you have designed and implemented. Additionally, include a comprehensive comparison with other compaction policies. Compare your algorithm with other compaction policies, including leveling, tiering, and lazy leveling in problem 1. Furthermore, adjust the alpha value in the benchmark (passing different alpha value to Part3Benchmark function), and determine the range of alpha where your algorithm performs the best. The parameters in Part3Benchmark(alpha, N, scan_length) are: alpha value, N is the number of keys, scan_length is the length of range scan. scan_length is set to be larger than N in this problem to ensure that all sorted runs are accessed. The read cost is still calculated by (number of sorted runs)*(block size) regardless of long scan_length . The read cost is calculated in the benchmark function as follows: read cost = 0 write cost = 0 for T in range(10): insert 10% of key-value pairs read cost += (number of sorted runs) * (block size) * (number of inserted key pairs) write cost += (write cost of inserted key pairs) read cost /= 10 write cost /= 10 total cost = (read cost) * alpha + (write cost) You can get points as long as your solution is reasonable and well-founded.","title":"Problem 2: find the best compaction policy (4pts)"},{"location":"p1/part3/#problem-3-find-the-best-compaction-policy-considering-range-filters-3pts","text":"Similar to bloom filters, there are also range filters. They can determine whether keys exist within a specified range in a sorted run. It allows range scans to skip sorted runs without relevant keys, optimizing query performance. We assume that we have a perfect range filter which does not produce false positives. Using this range filter, we can calculate the read cost based on the expected number of sorted runs that need to read. We also assume that the length m m of range scan is always the same. Let the total size of LSM-tree be S S . Suppose there is a sorted run of size T T in the LSM-tree. Assuming that all the keys in the LSM-tree are unique and uniformly distributed, for a range scan of length m m , the probability that a key from the sorted run will be in the result of the range scan is 1 - \\prod_{i=0}^{m-1}\\frac{S-T-i}{S-i} 1 - \\prod_{i=0}^{m-1}\\frac{S-T-i}{S-i} . You can consider for the first key in the result of the range scan, the probability that the key is not in the sorted run is \\frac{S-T}{S} \\frac{S-T}{S} , and for the second key it is \\frac{S-T-1}{S-1} \\frac{S-T-1}{S-1} , and so on. This probability is approximately \\approx 1-(1-\\frac{T}{S})^m\\approx 1-e^{-mT/S} \\approx 1-(1-\\frac{T}{S})^m\\approx 1-e^{-mT/S} . Then the read cost can be calculated by r(\\vec k)=(\\sum_{T} 1-e^{-mT/S})(block\\ size) r(\\vec k)=(\\sum_{T} 1-e^{-mT/S})(block\\ size) . The write cost is the same as problem 2. The code of estimation is in Part3Benchmark function. Your task: given the size of the last level N N , the base level size F F , the workload parameter alpha alpha , and the range scan length m m , approximate the read cost and write cost, find \\vec k, C \\vec k, C that minimize f(\\vec k, C) = w(\\vec k, C) + \\alpha r(\\vec k) f(\\vec k, C) = w(\\vec k, C) + \\alpha r(\\vec k) and satisfy N = \\prod_{i=1}^{L-1} k_i C F N = \\prod_{i=1}^{L-1} k_i C F . You need to implement it in FluidCompactionPicker::Get . We provide a basic benchmark that can be executed by test/test_lsm --gtest_filter=LSMTest.Part3Benchmark2 . Your algorithm should outperform than the baseline. There is no need to submit to autolab due to the long execution time. Please write a report detailing the algorithm you have designed and implemented. Additionally, include a comprehensive comparison with other compaction policies. Compare your algorithm with other compaction policies, including leveling, tiering, and lazy leveling in problem 1. Furthermore, adjust the alpha value in the benchmark, and determine the range of alpha where your algorithm performs the best. The parameters in Part3Benchmark(alpha, N, scan_length) are: alpha value, N is the number of keys, scan_length is the length of range scan. scan_length is set to 100 by default. You can also pass different scan_length to this function. The read cost is calculated as follows: read cost = 0 write cost = 0 for T in range(10): insert 10% of key-value pairs read cost += (number of sorted runs that need to be accessed) * (block size) * (number of inserted key pairs) write cost += (write cost of inserted key pairs) read cost /= 10 write cost /= 10 total cost = (read cost) * alpha + (write cost) You can get points as long as your solution is reasonable and well-founded.","title":"Problem 3: find the best compaction policy considering range filters (3pts)"}]}